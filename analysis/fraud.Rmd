---
title: "FraudScoreBenchmark"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

## Introduction

Real-time analytics is a class of problems characterized by a high volume of queries that require an
immediate response (< 1000 ms).

One example of a real-time analytics problem is credit card fraud scoring. In this case, a predictive
model might be trained and tested ahead of time, and then deployed to score the probability that each
new transaction is fraudulent. 

For this benchmark, we've borrowed a very simple fraud model from [Deployr](https://github.com/deployr/java-example-fraud-score-basics) and want to evaluate the performance
of a number options for deploying the model as a micro service that can queried to score transactions
in real-time.

We compare two options:
  * OpenCPU
  * Renjin, embedded in the Jetty HTTP server

We will evaluate these options according to several criteria:
  * Latency: the amount of time it takes to receive a response from the server
  * Throughput: the maximum number of requests per second the microservice can handle
  * Vertical Scalability: the increase in performance relative to increase in the number of cores
  

## Benchmarking 

We are using [Apache JMeter](http://jmeter.apache.org/) to load test the microservice.

The `benchmark.sh` script uses `terraform` and `jmeter` to automate the testing of each hardware
and load permutation:
  
  * Virtual machine with 4, 8, and 16 cores
  * 10, 50, 100, 200, and 500 concurrent requests

JMeter is set up to write the results of each permutation to the `results/` directory. 

## Processing the results



```{r process}

result.files <- list.files("../results", pattern = "*.csv")

sumtab <- data.frame()
for(i in seq_along(result.files)) {
  # Result files are stored in the following format:
  # (renjin|opencpu)-(cores)-(users).csv
  parts <- strsplit(result.files[i], split = ".", fixed = TRUE)[[1]]
  params <- strsplit(parts[[1]][1], split = "-", fixed = TRUE)[[1]]
  results <- read.csv(file.path("..", "results", result.files[i]))

  numRequests <- nrow(results)
  numSuccessfulRequests <- sum(results$success == "true")
  
  # For latency, find the median and the 95% percentile 
  latency <- quantile(results$Latency, c(0.5, 0.95))
  
  # Throughput (number of requests / total time)
  startTime <- results$timeStamp
  endTime <- startTime + results$elapsed
  runTime <- (max(endTime) - min(startTime)) / 1000
  throughput <- numSuccessfulRequests / runTime
  
  # Find the percentage of requests that failed
  successRate <- numSuccessfulRequests / numRequests
  
  sumtab[i, "solution"] <- params[1]
  sumtab[i, "cores"] <- as.integer(params[2])
  sumtab[i, "users"] <- as.integer(params[3])
  sumtab[i, "runtime"] <- runTime
  sumtab[i, "throughput"] <- throughput
  sumtab[i, "latency.median"] <- latency[1]
  sumtab[i, "latency.pctl95"] <- latency[2]
  sumtab[i, "success"] <- successRate
}

sumtab <- sumtab[ order(sumtab$solution, sumtab$cores, sumtab$users), ]
knitr::kable(sumtab)
```



## Computing Maximum Throughput

As the number of concurrent requests to a service increases, a service with fixed resources
passes through three phases: 

First, the throughput (number of requests per second) increases 
more or less in step with the number of concurrent requests as unused capacity is put to work.

Once available capacity has been exhausted, the throughput plateaus and the latency begins to rise. 
For the purpose of this benchmark, we consider the throughput at this plateau to be the
*maximum throughput*. 

Finally, once the server is saturated, additional concurrency can only be handled by queuing requests,
which leads to increasing latency. Eventually, as the costs of maintaining the queue rise, throughput
starts decreasing.

Looking at only the results from the 4-core, 16-GB virtual machine running on the Google Cloud
Platform, Renjin running embedded in the Jetty Server scales comfortable up to a peak throughput 
of 367 requests/second, while the same R code running in OpenCPU maxes out at 21 requests/second.

Even at the lowest tested level of load, with ten concurrent users, Renjin+Jetty responds with a
median latency of 31 ms, compared to a median latency of 465 ms by OpenCPU, a factor of 15.


``` {r sumtab4}
sumtab4 <- subset(sumtab, cores == 4)
```

```{r throughput_4core, echo=FALSE}

ggplot(sumtab4, aes(users, throughput, colour = solution)) + geom_line() +
  ggtitle("4-core") +
  xlab("Concurrent requests") +
  ylab("Throughput (reqs / second)")

```

```{r latency_4core, echo=FALSE}

ggplot(sumtab4, aes(users, latency.median, colour = solution)) + geom_line() +
  ggtitle("4-core") +
  xlab("Concurrent requests") +
  ylab("Median Latency (ms)")

```


## Vertical Scalability

Vertical scalability is the ability to increase the throughput of an application by increasing
the size of server, for example from 4 cores to 8 cores. 

```{r compute_max_throughput}
maxthru <- aggregate(throughput ~ solution + cores, data = sumtab, FUN = max)
knitr::kable(maxthru)
```

```{r max_thru_graph, echo=FALSE}

ggplot(maxthru, aes(cores, throughput, colour = solution)) + 
  geom_point() + geom_line() +
  ggtitle("Vertical scalability") +
  xlab("Cores") +
  ylab("Maximum Throughput (req/s)")

```
  
